{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMtBT6rDNodbhqpm3hJUins"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"xAoOm428kgV0"},"outputs":[],"source":["pip install gensim spacy torch scikit-learn\n"]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import MinMaxScaler\n","\n","# Charger la dataset (vérifiez le chemin et le nom du fichier extrait)\n","df = pd.read_csv(\"household_power_consumption.txt\",\n","                 sep=';',\n","                 parse_dates={'Datetime': ['Date', 'Time']},\n","                 infer_datetime_format=True,\n","                 na_values=['?'])\n","\n","# Pour cet exemple, nous nous concentrerons sur \"Global_active_power\"\n","# On peut supprimer les colonnes inutiles (ici, on garde \"Datetime\" et \"Global_active_power\")\n","df = df[['Datetime', 'Global_active_power']]\n","\n","# Supprimer les lignes avec des valeurs manquantes\n","df.dropna(inplace=True)\n","\n","# Trier par date (au cas où)\n","df.sort_values('Datetime', inplace=True)\n","\n","# Créer la cible : \"Global_active_power\" du jour suivant\n","# Pour simplifier, nous agrégeons les mesures par jour (par exemple, la moyenne journalière)\n","df_daily = df.resample('D', on='Datetime').mean()\n","df_daily['Target'] = df_daily['Global_active_power'].shift(-1)\n","df_daily = df_daily.dropna()  # Supprimer la dernière journée sans target\n","\n","# Normaliser les valeurs (ici, seule la colonne \"Global_active_power\" est normalisée)\n","scaler = MinMaxScaler()\n","df_daily[['Global_active_power']] = scaler.fit_transform(df_daily[['Global_active_power']])\n","\n","print(df_daily.head())\n"],"metadata":{"id":"-x0kr0-hlz6-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_sequences(data, time_steps=10):\n","    X, y = [], []\n","    values = data['Global_active_power'].values\n","    targets = data['Target'].values\n","    for i in range(len(values) - time_steps):\n","        X.append(values[i:(i+time_steps)])\n","        y.append(targets[i+time_steps])\n","    return np.array(X), np.array(y)\n","\n","time_steps = 10\n","X_seq, y_seq = create_sequences(df_daily, time_steps)\n","\n","print(\"Shape des séquences :\", X_seq.shape)  # (samples, time_steps)\n"],"metadata":{"id":"3GLTBvNhl08x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Division chronologique : 80% train, 10% validation, 10% test\n","total_samples = len(X_seq)\n","train_end = int(total_samples * 0.8)\n","val_end = int(total_samples * 0.9)\n","\n","X_train = X_seq[:train_end]\n","y_train = y_seq[:train_end]\n","X_val = X_seq[train_end:val_end]\n","y_val = y_seq[train_end:val_end]\n","X_test = X_seq[val_end:]\n","y_test = y_seq[val_end:]\n"],"metadata":{"id":"f_2W8QQVl3fp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","\n","class PowerDataset(Dataset):\n","    def __init__(self, X, y):\n","        # Reshape X pour avoir la forme (samples, time_steps, features)\n","        self.X = torch.tensor(X, dtype=torch.float32).unsqueeze(-1)  # ici features=1\n","        self.y = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n","\n","    def __len__(self):\n","        return len(self.X)\n","\n","    def __getitem__(self, idx):\n","        return self.X[idx], self.y[idx]\n","\n","batch_size = 32\n","train_dataset = PowerDataset(X_train, y_train)\n","val_dataset = PowerDataset(X_val, y_val)\n","test_dataset = PowerDataset(X_test, y_test)\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"],"metadata":{"id":"B6o4z2dvl6ky"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch.nn as nn\n","\n","class PowerPredictor(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers, dropout, output_size):\n","        super(PowerPredictor, self).__init__()\n","        self.gru = nn.GRU(input_size, hidden_size, num_layers, dropout=dropout, batch_first=True)\n","        self.dropout = nn.Dropout(dropout)\n","        self.fc = nn.Linear(hidden_size, output_size)\n","\n","    def forward(self, x):\n","        # x shape: (batch, time_steps, input_size)\n","        out, _ = self.gru(x)\n","        out = out[:, -1, :]  # dernière sortie de la séquence\n","        out = self.dropout(out)\n","        out = self.fc(out)\n","        return out\n","\n","input_size = 1  # Une seule feature (Global_active_power)\n","hidden_size = 50\n","num_layers = 2\n","dropout = 0.2\n","output_size = 1\n","\n","model = PowerPredictor(input_size, hidden_size, num_layers, dropout, output_size)\n","print(model)\n"],"metadata":{"id":"crKHm_pgl9bX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch.optim as optim\n","\n","criterion = nn.MSELoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n"],"metadata":{"id":"TacsKa76mQsG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import r2_score\n","\n","model.eval()\n","predictions = []\n","actuals = []\n","with torch.no_grad():\n","    for X_batch, y_batch in test_loader:\n","        outputs = model(X_batch)\n","        predictions.extend(outputs.squeeze().tolist())\n","        actuals.extend(y_batch.squeeze().tolist())\n","\n","r2 = r2_score(actuals, predictions)\n","print(\"R² score sur le test set :\", r2)\n"],"metadata":{"id":"K0PY9SfCmSl7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pickle\n","\n","with open('minmax_scaler.pkl', 'wb') as f:\n","    pickle.dump(scaler, f)\n"],"metadata":{"id":"U1h3LpuLmXZ3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"7hUjjTvwmaFQ"},"execution_count":null,"outputs":[]}]}